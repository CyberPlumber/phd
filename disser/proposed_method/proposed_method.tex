%!TEX root = ../dissertation.tex


\chapter{Proposed Method}
\label{method}

In theory, a perfect deterministic numerical forecast would consist of a perfect (in amplitude and phase) initialization and a perfect integration forward in time resulting in the correct forecast of what was observed.
Unfortunately, the current state of our modeling systems is such that perfect forecasts are impossible.
Numerical forecasts begin with errors in the initialization, use imperfect approximations of physical processes, and utilize discretized approximations of the continuous atmosphere, all of which result in errors in the final forecast.


These inherent limitations to numerical forecasts have led to the recognition that numerical forecasts should have a probabilistic component (e.g. \citealp{Glahn1972, Leith1974, Murphy1993, Glahn2009}).
For many years probabilistic guidance has been produced from deterministic forecasts in the United States using model output statistics (commonly referred to as MOS), a multivariate linear regression in which the likelihood of a specified outcome at a given location is estimated on the basis of past model performance \citep{Glahn1972}.
Unfortunately, statistical post-processing of model output works best when modeling systems allow for the creation of a forecast sample that adequately represents the larger forecast population.
In the case of operational numerical models this implies modeling systems must remain consistent for long periods of time.
Calibration becomes more challenging with modeling systems that have been recently modified, which limits the forecast sample.
Additional challenges arise when dealing with rare events, which require a long forecast sample to adequately represent the forecast population of rare events.


The statistical post-processing method proposed here uses some of the concepts developed by \cite{Theis2005} and \cite{Sobash2011}, but goes much further by employing a compositing and fitting technique for objective calibration of forecast probabilities.
The idea behind the proposed method is to objectively determine where observations of a given event occur relative to forecasts of that event.
This information is then used to fit an analytic function to the two-dimensional frequency histogram.
Several analytic functions might be good candidates for this purpose; a two-dimensional Gaussian function is used here, fitted using methods similar to \cite{Lak2010}.
In theory a normalized version of the raw two-dimensional frequency histogram, could be used, but, there is no guarantee that this would produce continuous forecast probabilities.
After fitting an analytic function to the two-dimensional histogram, this analytic function is used as the kernel to transform a forecast of ``yes''/``no'' occurrence into a continuous probabilistic forecast using methods similar to Kernel Density Estimation \citep{Silverman1986}.




\section{2-D Compositing}
\label{compositing}

The compositing employed here is straight forward.
First, forecasts are mapped onto the observational grid.
Next the raw forecasts and observations are converted into binary grids of 1s (predefined event occurred) and 0s (predefined event did not occur).
Next, all grid points within a specified radius of a forecast event are examined for observed events.
If an observed event occurs within the specified radius of a forecast, the position of the observation relative to the forecast is recorded.
Lastly, all of these relative positions are mapped onto a common grid and accumulated over all forecast periods during a specified training period.
The result is a two-dimensional frequency histogram of where observations occurred relative to forecasts during the entire training period.




\section{Fitting}
\label{fitting}

Once the two-dimensional frequency histogram is determined, the next step involves fitting an analytic function to the histogram.
As previously mentioned, any analytic function would work, as long it accurately approximates the two-dimensional frequency histogram.
A Gaussian function is demonstrated here.


To fit a Gaussian function, two parameters must be known: location and variance.
Typically, the location of a Gaussian function is given by the mean of the distribution.
In the case of a two-dimensional histogram this is given by the weighted center, or centroid, of the distribution.
Details on how to compute centroid are provided in subsection \ref{centroid}.
In the case of a two-dimensional Gaussian, the variance must be computed in both the X- and Y-dimensions.
Details on this are provided in subsection \ref{std}.




\subsection{Finding the 2-D Histogram's Centroid}
\label{centroid}

After creation of the two-dimensional frequency histogram, the next step involves determining the centroid of two-dimensional frequency histogram, and, in turn, determining the mean displacement vector.
The centroid's location, represented by $\mu_x$ and $\mu_y$ can be computed using

    \begin{equation}
        \label{mux}
        \mu_x = \frac{1}{\mathbf{H}} \sum\limits_{\substack{j=-m \\ i=-n}}^{n,m}H_{ij} \cdot i,
    \end{equation}

    \begin{equation}
        \label{muy}
        \mu_y = \frac{1}{\mathbf{H}} \sum\limits_{\substack{j=-m \\ i=-n}}^{n,m}H_{ij} \cdot j,
    \end{equation}

\noindent where $H$ is the observed two-dimensional histogram, $n$ is half the length of the x-dimension of $H$, $m$ is half the length of the y-dimension, and

    \begin{equation}
        \mathbf{H} = \sum\limits_{\substack{j=-m \\ i=-n}}^{n,m} H_{ij},
    \end{equation}

\noindent given that the forecast is found at $(0, 0)$.
Thus, the observation mean displacement vector is simply $(\mu_x, \mu_y)$.




\subsection{Finding the 2-D Histogram's Standard Deviation}
\label{std}

Once the centroid of the two-dimensional histogram is known, the next step to fitting a two-dimensional Gaussian function is to compute the variance in both dimensions.
The two-dimensional Gaussian function is represented mathematically as:

    \begin{equation}
        \label{2DGauss}
        G(x, y, \sigma_x, \sigma_y) = \frac{1}{2 \pi \sigma_x \sigma_y} e^{- \frac{1}{2} \left( \frac{x^2}{\sigma_x^2} + \frac{y^2}{\sigma_y^2} \right)},
    \end{equation}

\noindent where $\sigma_x$ is the standard deviation (or, bandwidth, when applied in a KDE framework) in the x-direction and $\sigma_y$ is the standard deviation in the y-direction.
When $\sigma_x \neq \sigma_y$, the Gaussian function is said to be anisotropic.
The function's peak probability density occurs at the center of the function, and decreases monotonically outward such that

    \begin{equation}
        \label{ellipse}
        \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1,
    \end{equation}

\noindent and

    \begin{equation}
        \frac{a}{b} = \frac{\sigma_x}{\sigma_y},
    \end{equation}

\noindent where $x$ is the x-component of the distance from the center, $y$ is the y-component of the distance from center, $a$ is the major axis, and $b$ is the minor axis, is satisfied.
The result is a set of concentric ellipses that monotonically decrease outward.
In the limiting case of $\sigma_x = \sigma_y$, the Gaussian function is said to be isotropic.
In this case, the function's peak probability density still occurs at the center of the function and decreases monotonically outward; however, \mbox{equation \ref{ellipse}} becomes

    \begin{equation}
        \frac{x^2 + y^2}{r} = 1,
    \end{equation}

\noindent where $r = \sqrt{a^2 + b^2}$.
In this case, concentric circles are produced rather than ellipses.

In \cite{Sobash2011} attempts to produce reliable forecasts centered on subjectively changing $\sigma_x$ for a two-dimensional isotropic Gaussian function until the value producing the most reliable forecasts was found\footnote{In a 2D isotropic Gaussian, $\sigma_x = \sigma_y$, thus \mbox{equation \ref{2DGauss}} is reduced to a form only depending on $\sigma_x$}.
Here, the proposed calibration method uses a two-dimensional anisotropic Gaussian function, with $\sigma_x$ and $\sigma_y$ objectively derived from the model's historical error characteristics.
This is done by using

    \begin{equation}
        \label{sigmax}
        \sigma_x^2 = \frac{1}{\mathbf{H}} \sum\limits_{\substack{j=-m \\ i=-n}}^{n,m}H_{ij}(i - \mu_x)^2,
    \end{equation}

    \begin{equation}
        \label{sigmay}
        \sigma_y^2 = \frac{1}{\mathbf{H}} \sum\limits_{\substack{j=-m \\ i=-n}}^{n,m}H_{ij}(j - \mu_y)^2,
    \end{equation}

\noindent which can then be used plugged into \mbox{equation \ref{2DGauss}}.

The two-dimensional Gaussian function represented by \mbox{equation \ref{2DGauss}} forces $\sigma_x$ and $\sigma_y$ to lie along the abscissa and ordinate, respectively.
In order to model a two-dimensional distribution where the major and minor axes are rotated off the abscissa and ordinate, a rotation matrix,

    \begin{equation}
        \label{RotationMatrix}
        \begin{bmatrix}
            x' \\ y'
        \end{bmatrix}
        =
        \begin{bmatrix}
            \cos{\theta} & -\sin{\theta} \\
            \sin{\theta} & \cos{\theta}
        \end{bmatrix}
        \begin{bmatrix}
            x \\ y
        \end{bmatrix},
    \end{equation}

\noindent where $\theta$ is the rotation angle (in radians) of the major axis from the abscissa in the counter-clockwise direction, must be applied to the two-dimensional Gaussian given in \mbox{equation \ref{2DGauss}}.


The rotation angle can be computed from the covariance matrix of the observed frequency distribution,

    \begin{equation}
        \label{CovMatrix}
        \boldsymbol{\Sigma}_{xy} =
        \begin{bmatrix}
            \sigma_x^2 & \sigma_{xy} \\
            \sigma_{xy} & \sigma_y^2
        \end{bmatrix}
    \end{equation}

\noindent where $\sigma_{xy}$ is the covariance of $x$ and $y$, and is given by

    \begin{equation}
        \label{sigmaxy}
        \sigma_{xy} = \frac{1}{\mathbf{H}} \sum\limits_{\substack{j=-m \\ i=-n}}^{n,m}H_{ij}(i - \mu_x)(j - \mu_y),
    \end{equation}

\noindent by taking the $\arctan{\left(\frac{y''}{x''}\right)}$, where $x''$ and $y''$ are the x- and y-components of the eigenvector of $\boldsymbol{\Sigma}_{xy}$ associated with the maximum eigenvalue \citep{Lak2010}.

The rotated, two-dimensional Gaussian equation can be represented mathematically by

    \begin{equation}
        \label{2DRotatedGauss}
        G'(x', y', \sigma_x, \sigma_y) = \frac{1}{2 \pi \sigma_x \sigma_y} e^{- \frac{1}{2} \left( \frac{x'^2}{\sigma_x^2} + \frac{y'^2}{\sigma_y^2} \right)},
    \end{equation}

\noindent where $x'$ and $y'$ are determined by \mbox{equation \ref{RotationMatrix}}.




\section{Putting it All Together}
\label{kde}

Once $\mu_x$, $\mu_y$, $\sigma_x$, $\sigma_y$, and $\theta$ are known, kernel density estimation can be carried out on any forecast.
This is done by creating a binary grid from the model's forecast, similar to the ones created in the compositing stage.
Each ``yes'' grid point (1) is then shifted by the mean displacement vector, ${\mu_x, \mu_y}$ and then smoothed by \mbox{equation \ref{2DRotatedGauss}}, using the objectively derived $\sigma_x$, $\sigma_y$, and $\theta$ out to a distance of $5\sigma_x$ in the x-direction and $5\sigma_y$ in the y-direction\footnote{$5\sigma_x$ and $5\sigma_y$, were chosen because this distance accounts for greater than 99.999\% of the fitted Gaussian distribution.
In practice, any multiple of $\sigma_x$ and $\sigma_y$ would work, with larger multiples accounting for more of the fitted Gaussian's tails.}.
This generates probabilities at the nearby grid points that the corresponding observation will actually occur there, rather than solely at the location of the forecast event.
The final forecast probabilities then become the linear combination of all the forecast probabilities created by applying the fitted Gaussian to each forecast event.
