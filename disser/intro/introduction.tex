%!TEX root = ../dissertation.tex


\chapter{Introduction}
\label{intro}

Rare meteorological events\footnote{\cite{Murphy1991} defined a rare meteorological event as one that occurs on less than five percent of forecasting occasions.} that occur on small spatial and short temporal scales pose significant challenges to forecasters.
This is related to the limited predictability of phenomena occurring on short time-space scales; however, these events comprise a substantial portion of meteorological phenomena that negatively impact society (e.g., heavy rain, large hail, tornadoes, etc.).
Thus, ``good'' forecasts of these events would provide large societal benefits.


Prior to the 20th century, weather forecasts were largely the result of rules of thumb, with little understanding for the physical mechanisms governing the atmosphere.
In 1901 Professor Cleveland Abbe proposed that the atmosphere was governed by a set of dynamic and thermodynamic equations, and that these equations could be used to predict future states of the atmosphere \citep{Abbe1901}.
\cite{Bjerknes1904} followed with the recognition that in order to produce forecasts of subsequent states of the atmosphere based on the governing equations:
\begin{enumerate}
    \item One has to know with sufficient accuracy the state of the atmosphere at a given time; and
    \item One has to know with sufficient accuracy the laws according to which one state of the atmosphere develops from another.
\end{enumerate}
Bjerknes recognized that the creation of a sufficiently accurate analysis of the atmosphere is a necessary condition for reliable forecasts of future atmospheric states based on the integration of the governing equations.


Beginning on the mid-1910s, Lewis Richardson took up the challenge of weather prediction and went on to produce the first numerical prediction (i.e. one using mathematical equations; \citealp{Lynch2008}).
The forecast took about \mbox{6 weeks} to produce by hand and predicted the change in surface pressure over the course of \mbox{6 hours}.
Although the forecast was a spectacular failure --- predicting a \mbox{145 mb} surface pressure fall when the barometric pressure remained nearly constant --- Richardson considered it ``a fairly correct deduction from a somewhat unnatural initial distribution,'' \citep{Lynch2008}.
Ultimately, Richardson's forecast was doomed from the start, falling victim to Bjerknes' first requirement for successful mathematical forecasts: adequate initial conditions.


In the late 1940s, Jule Charney demonstrated that larger-scale atmospheric motions could be sufficiently predicted by advecting the geostrophic vorticity with the geostrophic wind \citep{Charney1947}.
Charney, and his team of researchers, went on to produce the first successful numerical weather forecasts produced from a computer \cite{Charney1950}.
Word of this success rapidly spread throughout the meteorological community, and by the mid-1950s short-term operational numerical weather forecasts were being produced in the United States and Sweden \citep{Lewis2005}.







What makes a ``good'' forecast has been the subject of many discussions throughout the history of forecasting (e.g. \citealp{Mascart1922, Winkler1968, Murphy1993}, \citealp{Murphy1996} and papers within).
\cite{Murphy1993} provided three distinct measures of forecast ``goodness'': consistency, quality, and value.
Consistency, sometimes referred to as \mbox{type 1} ``goodness'', is a measure of correspondence between the actual forecast and the forecaster's judgment of what will occur.
Quality, sometimes referred to as \mbox{type 2} ``goodness'', is a measure of correspondence between the forecast and the observations.
Value, known as \mbox{type 3} ``goodness'', is a measure of the benefit a forecast provides to users of the forecast.


A natural consequence of maximizing consistency is the need for probabilistic forecasts.
This is the result of the uncertainty in a forecaster's judgment that must be conveyed in the resulting forecast.
When forecasters produce probabilistic forecasts that accurately depict their uncertainty, the best expected quality, as measured by a strictly proper scoring rules \citep{Winkler1968}, is also achieved.
Thus cementing the need for forecasts to have a probabilistic component.


Unfortunately, raw deterministic model output (i.e. output that has not been post-processed) is not probabilistic in nature.
Deterministic model output provides a precise answer at every grid point for all output fields.
This is a consequence of the internal numerics of deterministic models; equations are constrained to produce a single value.
However, raw deterministic model output can be thought of in terms of probabilities.
A single output value can be thought of having a probability of 1 that the output value will be observed, and a 0 that any other value will be observed.
Unless the deterministic model is always perfect, the probability that the model's forecast will be correct lies somewhere between \mbox{0 and 1}.
\cite{Lorenz1963, Lorenz1965} demonstrated that deterministic forecasts are subject to initially small errors that grow in amplitude until they become significant features of numerical forecasts.
These errors abound from all aspects of numerical weather prediction, ranging from numerical to observational to the model itself.
Thus, perfect, deterministic, numerical weather predictions are essentially impossible.
Therefore, in order for a numerical weather forecast to satisfy type 2 ``goodness'' they, too, must include a probabilistic component.


Unfortunately, computing capabilities have not always allowed for probabilistic numerical weather prediction.
The first operational numerical weather prediction models





Convection-allowing models (CAMs) have shown improved skill, compared to parameterized-convection models, in identifying regions where rare meteorological events associated with convection (hereafter RCEs\footnote{Rare Convective Event}) may occur \citep{Clark2010a}.
Furthermore, CAMs are able to do this by explicitly representing deep-convective storms and their unique attributes --- not just storm environments \citep{Kain2010}.
Yet, quantifying the uncertainty associated with explicit numerical prediction of RCEs is particularly challenging \citep{Sobash2011}.
Of course, ensembles are powerful tools for quantifying uncertainty, but when convection-allowing ensemble prediction systems are used to provide guidance for forecasting storm-attributes, they are subject to the same fundamental limitation that handicaps single-member CAMS forecast systems: Too little is known about the performance characteristics of CAMs in predicting RCEs explicitly.


There are three main reasons for this deficiency.
First, routine, explicit, contiguous  or near-contiguous United States (CONUS or near-CONUS) scale forecasts of RCEs have been available for only 6-7 years in the United States, so there is still much to learn about which phenomena can be skillfully predicted with convection-allowing models \citep{Kain2008, Kain2010}.
Second, most real-time forecasting efforts with convection-allowing models have been short-term initiatives, focusing on specific tasks (e.g., \citealp{Done2004, Weisman2008}).
Third, there is a limited database of forecasts for RCEs, making robust statistical techniques difficult (e.g., \citealp{Hamill2006}).
In short, there is a limited track record in the use of CAMs as guidance for prediction of RCEs.


A strategy for calibrating, or quantifying the uncertainty of, forecasts of RCEs based on the idea of generating probabilistic forecasts from a single underlying deterministic model follows.
This technique uses a conceptual approach similar to that described by \cite{Theis2005} and refined by \cite{Sobash2011}.
As in these two studies, this strategy differs from other methods for both deterministic models (e.g., \citealp{Glahn1972}) and ensemble modeling systems (e.g., \citealp{Hamill1998, Raftery2005, Clark2009, Glahn2009}) by including a neighborhood around each model grid point as a fundamental component of the calibration process.


This strategy is rooted in the fundamental concepts of Kernel Density Estimation (KDE), which can be used to retrieve spatial probability distributions from point observations, or, in this case, forecasts.
In other words, if a model forecasts an event at point A, KDE can be utilized to gain insight into the probability that the event might occur at a nearby point.
This is achieved by utilizing a statistical distribution to redistribute the total probability (100\%) from point A over multiple (typically nearby) grid points.
The result is a probability forecast, the character of which is determined by one's choice of statistical distribution and the number of grid points over which the distribution is applied.
The resulting smoothing effect is similar to that obtained with ensemble output by \cite{Wilks2002}, but initial calibration efforts herein focus on output from a single deterministic model.
\cite{Sobash2011} demonstrated with a two-dimensional, isotropic Gaussian function that calibration of the probability forecasts derived using this technique is most easily done by changing the number of grid points over which non-zero probabilities are distributed.
Here, however, an objective calibration method based on past model performance, is presented.


This dissertation is laid out as follows: The method is presented in Chapter \ref{method}, followed by application of the method to a deterministic model in Chapter \ref{deterministic}.
Extension to ensembles is presented in Chapter \ref{ensemble} and an overall discussion concludes the dissertation in Chapter \ref{discussion}.
An explanation of the data used for both the deterministic and ensemble methods can be found in their respective chapters.


