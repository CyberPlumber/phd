%!TEX root = ../dissertation.tex


\chapter{Introduction}
\label{intro}

Rare meteorological events\footnote{\cite{Murphy1991} defined a rare meteorological event as one that occurs on less than five percent of forecasting occasions.} that occur on small spatial and short temporal scales pose significant challenges to forecasters.
This is related to the limited predictability of phenomena occurring on short time-space scales; however, these events comprise a substantial portion of meteorological phenomena that negatively impact society (e.g., heavy rain, large hail, tornadoes, etc.).
Thus, ``good'' forecasts of these events would provide large societal benefits.


What makes a ``good'' forecast has been the subject of many discussions throughout the history of forecasting (e.g. \citealp{Peirce1884, Clayton1889, Nichols1890, Mascart1922, Winkler1968, Murphy1993, Murphy1996} and papers therein).
In an essay designed to address this question, \cite{Murphy1993} provided three distinct measures of forecast ``goodness'': consistency, quality, and value.
Consistency, sometimes referred to as \mbox{type 1} ``goodness'', is a measure of correspondence between the actual forecast and the forecaster's judgment of what will occur.
Quality, sometimes referred to as \mbox{type 2} ``goodness'', is a measure of correspondence between the forecast and the observations.
Value, known as \mbox{type 3} ``goodness'', is a measure of the benefit a forecast provides to users of the forecast.


A natural consequence of maximizing consistency is the need for probabilistic forecasts.
This is the result of the uncertainty in a forecaster's judgment that is often the result of uncertainty in forecast guidance and thus must be conveyed in the resulting forecast.
When forecasters produce probabilistic forecasts that accurately depict their uncertainty, the best expected quality, as measured by a strictly proper scoring rules \citep{Winkler1968}, is also achieved, cementing the need for forecasts to have a probabilistic component.


Prior to the twentieth century, weather forecasts were largely the result of rules of thumb, with little understanding for the physical mechanisms governing the atmosphere.
However, several attempts at utilizing probabilities (and odds) in weather forecasts date back to the late eighteenth century when J. Dalton included measures of uncertainty in his forecasts \citep{Dalton1793, Murphy1998}.
Although the exact forecast methods of Dalton are not known, Dalton's forecasts include statements such as ``the probability of rain was much smaller than at other times'' and ``the probability of a fair day to that of a wet one is as ten to one,'' \citep{Murphy1998}.
Nearly a century later, in 1871, under the direction of Professor Cleveland Abbe, the first forecasts and warnings issued by the United States Signal Service were actually labeled probabilities. \citep{Whitnah1961, Murphy1998}.
In fact, Professor Abbe promoted the use of ``probabilities'' so much he earned the nickname ``Old Probabilities'' \citep{Scott1873, Murphy1998}.


Although probability forecasting of some variety was performed prior to the beginning of the twentieth century, the start of probability forecasting is often attributed to the work of W. E. Cooke \citep{Murphy1998}.
\cite{Cooke1906a} eloquently summarized the shortcomings of deterministic forecasting, and championed the inclusion of uncertainty information, by stating:
\begin{quote}
    ``(a)ll those whose duty it is to issue regular daily forecasts know that there are times when they feel very confident and other times when they are doubtful as to the coming weather. It seems to me that the condition of confidence or otherwise forms a very important part of the prediction, and ought to find expression. It is not fair to the forecaster that equal weight should be assigned to all his predictions and the usual method tends to retard that public confidence which all practical meteorologists desire to foster. It is more scientific and honest to be allowed occasionally to say `I feel very doubtful about the weather for tomorrow'\dots and it must be\dots useful to the public if one is allowed occasionally to say `It is practically certain that the weather will be so-and-so tomorrow''' (\mbox{p. 23}; \citealp{Murphy1998}).
\end{quote}


\noindent\cite{Cooke1906a} went on to propose a forecasting paradigm in which a forecaster assigned a set of uncertainty weights to each forecast.
The uncertainty weights were designed to express various levels of confidence, and it was shown that forecasters were were able to assign weights that appropriately reflected their uncertainty.
Although these forecasts conveyed uncertainty information, they were not fully probabilistic in nature.
It wasn't until \cite{Hallenbeck1920} that numerical probabilistic forecasts were recorded in the literature.


Around the same time as the work done by Hallenbeck, a series of papers by Anders {{\AA}}ngstr{{\"o}}m poignantly made the case for probabilistic rather than binary or categorical warnings \citep{Angstrom1919, Angstrom1922, Liljas1994, Murphy1998}.
The crux of {{\AA}}ngstr{{\"o}}m's arguments revolved around the difficulties forecasters face when they are constrained to issue a warning in binary terms.
This difficulty, argued {{\AA}}ngstr{{\"o}}m, arises from the fact that forecasters' knowledge of users' cost/loss ratios is generally insufficient to determine whether to ``issue a warning" or ``not issue a warning" in the context of producing the best possible forecast for users.
{{\AA}}ngstr{{\"o}}m further noted that ``(w)hat makes the matter still more complicated is the fact that\dots [the loss function]\dots has very different values in different cases," \citep{Angstrom1922, Murphy1998}. {{\AA}}ngstr{{\"o}}m ultimately concluded: ``The most appropriate system seems therefore to be to leave to the clients concerned by the warning to form an idea of the ratio [cost-loss ratio]\dots and to issue the warnings in such a form that the larger or smaller probability of the event gets clear from the formulation.
The client may then himself consider if it is worthwhile to make arrangements of protection or to disregard a given warning," \citep{Angstrom1922, Murphy1998}.
In these two short papers, {{\AA}}ngstr{{\"o}}m succinctly argued the case for probability forecasts.
Unfortunately, advancements in probability forecasting lay dormant until mid-century.


In addition to probability forecasting, the start of the twentieth century marked the conception of numerical weather prediction.
In 1901 Professor Abbe proposed that the atmosphere was governed by a set of dynamic and thermodynamic equations, and that these equations could be used to predict future states of the atmosphere \citep{Abbe1901}.
\cite{Bjerknes1904} followed with the recognition that in order to produce forecasts of subsequent states of the atmosphere based on the governing equations:
\begin{enumerate}
    \item One has to know with sufficient accuracy the state of the atmosphere at a given time; and
    \item One has to know with sufficient accuracy the laws according to which one state of the atmosphere develops from another.
\end{enumerate}
Bjerknes recognized that the creation of a sufficiently accurate analysis of the atmosphere is a necessary condition for reliable forecasts of future atmospheric states based on the integration of the governing equations.


Lewis Richardson took up the challenge of weather prediction in the 1910s and went on to produce the first numerical prediction (i.e. one using mathematical equations; \citealp{Lynch2008}).
The forecast took about \mbox{6 weeks} to produce by hand and predicted the change in surface pressure over the course of \mbox{6 hours}.
Although the forecast was a spectacular failure --- predicting a \mbox{145 mb} surface pressure fall when the barometric pressure remained nearly constant --- Richardson considered it ``a fairly correct deduction from a somewhat unnatural initial distribution,'' \citep{Lynch2008}.
Ultimately, Richardson's forecast was doomed from the start, falling victim to Bjerknes' first requirement for successful numerical forecasts: adequate initial conditions.


In the late 1940s, Jule Charney demonstrated that larger-scale atmospheric motions could be sufficiently predicted by advecting the geostrophic vorticity with the geostrophic wind \citep{Charney1947}.
A few years later, Charney, and his team of researchers, went on to produce the first successful numerical weather forecasts produced from a computer \citep{Charney1950}.
Word of this success spread rapidly throughout the meteorological community, and by the mid-1950s short-term operational numerical weather forecasts were being produced in the United States and Sweden \citep{Lewis2005}.
Although operational numerical forecasts began in the mid 1950s, these forecasts were based on the simplified equations formulated by \cite{Charney1947}.
As a result of the simplified equations used in numerical weather forecasts, some meteorologists began to explore statistical methods to correct the numerical output \citep{Gleeson1961}.
Based on the work of \cite{Hinkelmann1951}, operational numerical forecasts began using the primitive equations to produce numerical forecasts in the mid 1960s \citep{Lynch2008}.


Unfortunately, raw deterministic model output (i.e. output that has not been post-processed) is not probabilistic in nature.
Deterministic model output provides a precise answer at every grid point for all output fields.
This is a consequence of the internal numerics of deterministic models; equations are constrained to produce a single value.
However, raw deterministic model output can still be thought of in terms of probabilities.
A single output value can be interpreted as having a probability of 1 that the output value will be observed, and a 0 that any other value will be observed.
Unless the deterministic model is always perfect (or always wrong), the probability that the model's forecast will be correct lies somewhere between \mbox{0 and 1}.


In the 1960s, Edward Lorenz published a series of papers that demonstrated the chaotic nature of the atmosphere \citep{Lorenz1963, Lorenz1965, Lorenz1968}.
Lorenz demonstrated that even small errors in the initial state of the modeled atmosphere --- even those within observational error --- can have large impacts on the resulting numerical forecast.
These errors abound from all aspects of numerical weather prediction, ranging from numerical to observational to the model itself.
Thus, perfect, deterministic, numerical weather predictions are practically impossible.
Therefore, in order for a numerical weather forecast to satisfy type 2 ``goodness'' they, too, must include a probabilistic component.


Armed with the results of \cite{Lorenz1963, Lorenz1965, Lorenz1968}, alternatives to the deterministic numerical prediction paradigm began to be offered.
\cite{Epstein1969} recognized that the atmosphere could not be completely described with a single numerical forecast due to inherent uncertainty.
\cite{Epstein1969} and \cite{Gleeson1970} each offered ways to produce probability forecasts in which the numerical model output means and variances directly.
Although these methods showed promise, the addition of uncertainty terms proved to be too computationally expensive for operational use at that time.


\cite{Leith1974} offered the first attempt at Monte Carlo forecasting, or ensemble forecasting as it is known today.
Leith suggested that instead of running one forecast from a single initialization, a collection (ensemble) of forecasts, each with a slightly different initial state, should be produced.
The resulting forecasts could be used to assess the uncertainty of the forecast.
One such method of assessing the uncertainty is to report the fraction of ensemble members that produce a certain outcome divided by the total number of ensemble members.
This results in a probability of occurrence, derived from the collection of numerical forecasts.
As was the case with \cite{Epstein1969} and \cite{Gleeson1970}, even though this approach showed promise, it was too expensive for operational use at that time.
It took nearly two decades worth of computational advancement, but operational ensemble prediction systems became a reality in the early 1990s.


In the mean time, a different approach was put forth by \cite{Glahn1972}.
Instead of adding additional terms to the numerical model, or producing multiple model forecasts each starting with slightly different initial conditions, the idea was to post-process the raw model output using statistical models.
The statistical models are in essence multi-variate regressions, derived from the three-dimensional numerical output, observations, and the general climatological conditions for specific locations.
These model output statistics, or MOS, account for model biases as well as local effects that cannot be resolved by the native resolution of the model.
Although some MOS products are generally expressed deterministically (such as the maximum and minimum \mbox{6 hr} temperatures), some products are probabilistic (such as probability of precipitation).
MOS is still used today by the National Weather Service \citep{Allen2001a, Allen2001b, Sfanos2001, Carroll2005, Glahn2009}.


As previously alluded, during the latter parts of the twentieth century computing power rapidly increased.
This led to a debate about whether extra resources should be devoted to increasing resolution or introducing ensembles.
Operational numerical weather prediction centers committed most of the additional resources to decreasing model grid spacing (e.g, \citealp{McPherson1991, WMO1992}).


The choice to embrace higher resolution numerical models resulted in much needed improvements to the models' physics parameterizations.
Additionally, improved model grid spacing allowed for numerical models to better represent atmospheric phenomenon with small dimension, strong gradients, or both (i.e., fronts, drylines, inversions, etc.).
These improvements allowed for highly specific spatial and temporal forecasts to be easily produced \citep{Lilly1990}.
Unfortunately, the details in high resolution forecasts are often the least skillful aspects.
These details are best in environments in which the atmosphere is dominated by large-scale phenomenon (i.e., when quasi-geostrophy can be assumed; \citealp{Antolik1989}).
Thus, numerical guidance tends to be at its best when the forecasting situation is, in some sense, the easiest, and is least needed \citep{Brooks1993}.


Probabilistic prediction becomes more important as the resolution of models increases, thus ensemble prediction concepts continued to receive attention if not operational implementation. (e.g. \citealp{Brooks1992a, Brooks1993}).
As previously noted, \cite{Lorenz1963, Lorenz1965, Lorenz1968} demonstrated that a numerical weather forecast was sensitive to the initial conditions.
Unfortunately, model grid spacing is typically less than that at which we observe the atmosphere over the globe.
Thus, in addition to the errors introduced by the observing instruments, additional errors can be introduced as a result of interpolating observations to grid points that do not have collocated observations.
A consequence of this is that the true state of the atmosphere is never precisely known.


Numerical weather prediction, and in particular, high-resolution models of thunderstorms, has been shown to be very sensitive to small changes in the initial conditions \citep{Lorenz1963, Lorenz1965, Lorenz1968, Brooks1992a, Brooks1992b}.
As a result, the inherent uncertainty in the observations could result in major errors in a forecast from models on that scale.
An ensemble approach recognizes the uncertainty in the initial conditions and utilizes it to produce a collection of forecasts, rather than assuming that the initialization is correct \citep{Brooks1992a}.


History has shown that a compromise of both of the increased resolution and ensemble forecasting paradigms has prevailed.
Operational centers in the United States currently run a global ensemble at relatively coarse resolution, whereas a suite of limited area models are run at increasingly higher resolution as the domain size decreases.
Recently, the computational power has increased to the point of allowing operational limited area model forecasts at grid spacings small enough for the cumulus parameterization scheme to be turned off.
Even with the sensitivities to initial conditions previously mentioned, these convection-allowing models (CAMs) have shown improved skill, compared to parameterized-convection models, in identifying regions where rare meteorological events associated with convection (hereafter RCEs\footnote{Rare Convective Event}) may occur \citep{Clark2010a}.
Furthermore, CAMs are able to do this by explicitly representing deep-convective storms and their unique attributes --- not just storm environments \citep{Kain2010}.


Yet, as promising as CAM forecasts are, quantifying the uncertainty associated with explicit numerical prediction of RCEs is particularly challenging \citep{Sobash2011}.
Of course, ensembles are powerful tools for quantifying uncertainty, but when convection-allowing ensemble prediction systems are used to provide guidance for forecasting storm-attributes, they are subject to the same fundamental limitation that handicaps single-member CAMS forecast systems: Too little is known about the performance characteristics of CAMs in predicting RCEs explicitly.


There are three main reasons for this deficiency.
First, routine, explicit, contiguous  or near-contiguous United States (CONUS or near-CONUS) scale forecasts of RCEs have been available for nearly a decade in the United States, but there is still much to learn about which phenomena can be skillfully predicted with convection-allowing models \citep{Kain2008, Kain2010}.
Second, most real-time forecasting efforts with convection-allowing models have been short-term initiatives, focusing on specific tasks (e.g., \citealp{Done2004, Weisman2008}).
Third, there is a limited database of forecasts for RCEs, making robust statistical techniques difficult (e.g., \citealp{Hamill2006}).
In short, there is a limited track record in the use of CAMs as guidance for prediction of RCEs.


A strategy for calibrating, or quantifying the uncertainty of, forecasts of RCEs based on the idea of generating probabilistic forecasts from a single underlying deterministic model follows.
This technique uses a conceptual approach similar to that described by \cite{Theis2005} and refined by \cite{Sobash2011}.
As in these two studies, this strategy differs from other methods for both deterministic models (e.g., \citealp{Glahn1972}) and ensemble modeling systems (e.g., \citealp{Hamill1998, Raftery2005, Clark2009, Glahn2009}) by including a neighborhood around each model grid point as a fundamental component of the calibration process.


This strategy is rooted in the fundamental concepts of Kernel Density Estimation (KDE), which can be used to retrieve spatial probability distributions from point observations, or, in this case, forecasts.
In other words, if a model forecasts an event at point A, KDE can be utilized to gain insight into the probability that the event might occur at a nearby point.
This is achieved by utilizing a statistical distribution to redistribute the total probability (100\%) from point A over multiple (typically nearby) grid points.
The result is a probability forecast, the character of which is determined by one's choice of statistical distribution and the number of grid points over which the distribution is applied.
The resulting smoothing effect is similar to that obtained with ensemble output by \cite{Wilks2002}, but initial calibration efforts herein focus on output from a single deterministic model.
\cite{Sobash2011} demonstrated with a two-dimensional, isotropic Gaussian function that calibration of the probability forecasts derived using this technique is most easily done by changing the number of grid points over which non-zero probabilities are distributed.
Here, however, an objective calibration method based on past model performance is presented.


The layout of this dissertation is as follows: The method is presented in Chapter \ref{method}, followed by application of the method to a deterministic model in Chapter \ref{deterministic}.
Extension to ensembles is presented in Chapter \ref{ensemble} and an overall discussion concludes the dissertation in Chapter \ref{discussion}.
An explanation of the data used for both the deterministic and ensemble methods can be found in their respective chapters.
