%!TEX root = ../dissertation.tex


\chapter{Introduction}
\label{intro}

Rare meteorological events\footnote{\cite{Murphy1991} defined a rare meteorological event as one that occurs on less than five percent of forecasting occasions.} that occur on small spatial and short temporal scales pose significant challenges to forecasters.
This is related to the limited predictability of phenomena occurring on short time-space scales; however, these events comprise a substantial portion of meteorological phenomena that negatively impact society (e.g., heavy rain, large hail, tornadoes, etc.).
Thus, ``good'' forecasts of these events would provide large societal benefits.


What makes a ``good'' forecast has been the subject of many discussions throughout the history of forecasting (e.g. \citealp{Clayton1889, Nichols1890, Mascart1922, Winkler1968, Murphy1993, Murphy1996} and papers therein).
In an essay designed to address this question, \cite{Murphy1993} provided three distinct measures of forecast ``goodness'': consistency, quality, and value.
Consistency, sometimes referred to as \mbox{type 1} ``goodness'', is a measure of correspondence between the actual forecast and the forecaster's judgment of what will occur.
Quality, sometimes referred to as \mbox{type 2} ``goodness'', is a measure of correspondence between the forecast and the observations.
Value, known as \mbox{type 3} ``goodness'', is a measure of the benefit a forecast provides to users of the forecast.


A natural consequence of maximizing consistency is the need for probabilistic forecasts.
This is the result of the uncertainty in a forecaster's judgment that must be conveyed in the resulting forecast.
When forecasters produce probabilistic forecasts that accurately depict their uncertainty, the best expected quality, as measured by a strictly proper scoring rules \citep{Winkler1968}, is also achieved, cementing the need for forecasts to have a probabilistic component.




\section{A Brief History of Probability Forecasting}

Prior to the twentieth century, weather forecasts were largely the result of rules of thumb, with little understanding for the physical mechanisms governing the atmosphere.
However, several attempts at utilizing probabilities (and odds) in weather forecasts date back to the late eighteenth century \citep{Dalton1793, Murphy1998}.
Although the exact forecast methods of Dalton are not known, Dalton's forecasts include statements such as ``the probability of rain was much smaller than at other times'' and ``the probability of a fair day to that of a wet one is as ten to one,'' \citep{Murphy1998}.
Nearly a century later, in 1871, under the direction of Professor Cleveland Abbe, the first forecasts and warnings issued by the United States Signal Service were actually labeled probabilities. \citep{Whitnah1961, Murphy1998}.
In fact, Professor Abbe promoted the use of ``probabilities'' so much he earned the nickname ``Old Probabilities'' \citep{Scott1873, Murphy1998}.

\cite{Cooke1906a, Cooke1906b}


In 1901 Professor Abbe proposed that the atmosphere was governed by a set of dynamic and thermodynamic equations, and that these equations could be used to predict future states of the atmosphere \citep{Abbe1901}.
\cite{Bjerknes1904} followed with the recognition that in order to produce forecasts of subsequent states of the atmosphere based on the governing equations:
\begin{enumerate}
    \item One has to know with sufficient accuracy the state of the atmosphere at a given time; and
    \item One has to know with sufficient accuracy the laws according to which one state of the atmosphere develops from another.
\end{enumerate}
Bjerknes recognized that the creation of a sufficiently accurate analysis of the atmosphere is a necessary condition for reliable forecasts of future atmospheric states based on the integration of the governing equations.


When forecasters produce probabilistic forecasts that accurately depict their uncertainty, the best expected quality, as measured by a strictly proper scoring rules \citep{Winkler1968}, is also achieved.







Lewis Richardson took up the challenge of weather prediction in the 1910s and went on to produce the first numerical prediction (i.e. one using mathematical equations; \citealp{Lynch2008}).
The forecast took about \mbox{6 weeks} to produce by hand and predicted the change in surface pressure over the course of \mbox{6 hours}.
Although the forecast was a spectacular failure --- predicting a \mbox{145 mb} surface pressure fall when the barometric pressure remained nearly constant --- Richardson considered it ``a fairly correct deduction from a somewhat unnatural initial distribution,'' \citep{Lynch2008}.
Ultimately, Richardson's forecast was doomed from the start, falling victim to Bjerknes' first requirement for successful numerical forecasts: adequate initial conditions.


In the late 1940s, Jule Charney demonstrated that larger-scale atmospheric motions could be sufficiently predicted by advecting the geostrophic vorticity with the geostrophic wind \citep{Charney1947}.
A few years later, Charney, and his team of researchers, went on to produce the first successful numerical weather forecasts produced from a computer \citep{Charney1950}.
Word of this success spread rapidly throughout the meteorological community, and by the mid-1950s short-term operational numerical weather forecasts were being produced in the United States and Sweden \citep{Lewis2005}.
Although operational numerical forecasts began in the mid 1950s, these forecasts were based on the simplified equations formulated by \cite{Charney1947}.
As a result of the simplified equations used in numerical weather forecasts, some meteorologists began to explore statistical methods to correct the numerical output \citep{Gleeson1961}.
Based on the work of \cite{Hinkelmann1951}, operational numerical forecasts began using the primative equaitons to produce numerical forecasts in the mid 1960s \citep{Lynch2008}.


Unfortunately, raw deterministic model output (i.e. output that has not been post-processed) is not probabilistic in nature.
Deterministic model output provides a precise answer at every grid point for all output fields.
This is a consequence of the internal numerics of deterministic models; equations are constrained to produce a single value.
However, raw deterministic model output can still be thought of in terms of probabilities.
A single output value can be interpreted as having a probability of 1 that the output value will be observed, and a 0 that any other value will be observed.
Unless the deterministic model is always perfect, the probability that the model's forecast will be correct lies somewhere between \mbox{0 and 1}.


In the 1960s, Edward Lorenz published a series of papers that demonstrated the chaotic nature of the atmosphere \citep{Lorenz1963, Lorenz1965, Lorenz1968}.
Lorenz demonstrated that even small errors in the initial state of the modeled atmosphere --- even those within observational error --- can have large impacts on the resulting numerical forecast.
These errors abound from all aspects of numerical weather prediction, ranging from numerical to observational to the model itself.
Thus, perfect, deterministic, numerical weather predictions are essentially impossible.
Therefore, in order for a numerical weather forecast to satisfy type 2 ``goodness'' they, too, must include a probabilistic component.


Armed with the results of \cite{Lorenz1963, Lorenz1965, Lorenz1968}, alternatives to the deterministic numerical prediction paradigm began to be offered.
\cite{Epstein1969} recognized that the atmosphere could not be completely described with a single numerical forecast due to inherent uncertainty.
\cite{Epstein1969} and \cite{Gleeson1970} each offered ways to produce probability forecasts in which the numerical model output means and variances directly.
Although these methods showed promise, the addition of uncertainty terms proved to be too computationally expensive for operational use at that time.


\cite{Leith1974} offered the first attempt at Monte Carlo forecasting, or ensemble forecasting as it is known today.
Leith suggested that instead of running a single forecast from a single initialization, a collection (ensemble) of forecasts, each with a slightly different initial state, should be produced.
The resulting forecasts could be used to assess the uncertainty of the forecast.
As was the case with \cite{Epstein1969} and \cite{Gleeson1970}, even though this approach showed promise, it was too expensive for operational use at that time.
It took nearly two decades worth of computational advancement, but operational ensemble prediction systems because a reality in the early 1990s.









\section{Convection Allowing Models}

Convection-allowing models (CAMs) have shown improved skill, compared to parameterized-convection models, in identifying regions where rare meteorological events associated with convection (hereafter RCEs\footnote{Rare Convective Event}) may occur \citep{Clark2010a}.
Furthermore, CAMs are able to do this by explicitly representing deep-convective storms and their unique attributes --- not just storm environments \citep{Kain2010}.
Yet, quantifying the uncertainty associated with explicit numerical prediction of RCEs is particularly challenging \citep{Sobash2011}.
Of course, ensembles are powerful tools for quantifying uncertainty, but when convection-allowing ensemble prediction systems are used to provide guidance for forecasting storm-attributes, they are subject to the same fundamental limitation that handicaps single-member CAMS forecast systems: Too little is known about the performance characteristics of CAMs in predicting RCEs explicitly.


There are three main reasons for this deficiency.
First, routine, explicit, contiguous  or near-contiguous United States (CONUS or near-CONUS) scale forecasts of RCEs have been available for only 6-7 years in the United States, so there is still much to learn about which phenomena can be skillfully predicted with convection-allowing models \citep{Kain2008, Kain2010}.
Second, most real-time forecasting efforts with convection-allowing models have been short-term initiatives, focusing on specific tasks (e.g., \citealp{Done2004, Weisman2008}).
Third, there is a limited database of forecasts for RCEs, making robust statistical techniques difficult (e.g., \citealp{Hamill2006}).
In short, there is a limited track record in the use of CAMs as guidance for prediction of RCEs.


A strategy for calibrating, or quantifying the uncertainty of, forecasts of RCEs based on the idea of generating probabilistic forecasts from a single underlying deterministic model follows.
This technique uses a conceptual approach similar to that described by \cite{Theis2005} and refined by \cite{Sobash2011}.
As in these two studies, this strategy differs from other methods for both deterministic models (e.g., \citealp{Glahn1972}) and ensemble modeling systems (e.g., \citealp{Hamill1998, Raftery2005, Clark2009, Glahn2009}) by including a neighborhood around each model grid point as a fundamental component of the calibration process.


This strategy is rooted in the fundamental concepts of Kernel Density Estimation (KDE), which can be used to retrieve spatial probability distributions from point observations, or, in this case, forecasts.
In other words, if a model forecasts an event at point A, KDE can be utilized to gain insight into the probability that the event might occur at a nearby point.
This is achieved by utilizing a statistical distribution to redistribute the total probability (100\%) from point A over multiple (typically nearby) grid points.
The result is a probability forecast, the character of which is determined by one's choice of statistical distribution and the number of grid points over which the distribution is applied.
The resulting smoothing effect is similar to that obtained with ensemble output by \cite{Wilks2002}, but initial calibration efforts herein focus on output from a single deterministic model.
\cite{Sobash2011} demonstrated with a two-dimensional, isotropic Gaussian function that calibration of the probability forecasts derived using this technique is most easily done by changing the number of grid points over which non-zero probabilities are distributed.
Here, however, an objective calibration method based on past model performance, is presented.


This dissertation is laid out as follows: The method is presented in Chapter \ref{method}, followed by application of the method to a deterministic model in Chapter \ref{deterministic}.
Extension to ensembles is presented in Chapter \ref{ensemble} and an overall discussion concludes the dissertation in Chapter \ref{discussion}.
An explanation of the data used for both the deterministic and ensemble methods can be found in their respective chapters.


