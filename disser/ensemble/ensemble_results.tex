%!TEX root = ../dissertation.tex


\section{Ensemble Results}
\label{eresults}




\subsection{25.4 mm Threshold}
\label{eresults_25.4mm}

As previously mentioned, the first step of the proposed calibration process is to create two-dimensional composites of where observations occurred relative to forecast grid points.
In this case that means creating two-dimensional composites for each member, for each of the twenty simulations.
Visualizing, analyzing, and understanding all of these two-dimensional composites is challenging as there are 300 two-dimensional composites.
To achieve this goal, a two-step approach was taken: 1) analyze each SSEF member's distribution for each the five Gaussian fitting parameters; and 2) examine aggregate measures of the spatial distributions.


A set of five figures (\mbox{Figures \ref{sigmax-25mm-400km-dist}--\ref{mu_y-25mm-400km-dist}}) were created to examine the variability of each of the five Gaussian fitting parameters --- one figure per fitting parameter.
The figures contain box-and-whisker plots for each member's distribution of that figure's fitting parameter.
The box-and-whisker plots offer insight into the variability in the various fitting functions used by each member.
In each of the figures, the red horizontal line marks the median value of the distributions, and the blue box represents the 25th and 75th percentiles.
The whiskers denote the range of the distribution, up to $\pm$ 1.5 times the interquartile range.
Gold stars are used to denote any outliers, defined to be any value of the distribution that is outside the range of $\pm$ 1.5 times the interquartile range.


\mbox{Figure \ref{sigmax-25mm-400km-dist}} displays the box-and-whisker plots for the $\sigma_x$ fitting parameter.
This parameter is the length of the long axis of the fitted anisotropic Gaussian function.
Nine out of the fifteen ensemble members do not have outliers.
Of the remaining six ensemble members with outliers, two of them only have one outlier, two of them have two outliers, and one each has four and five outliers.
Ensemble members that have outliers have a range of over 20 kilometers for $\sigma_x$, whereas those members without outliers generally have ranges of less than 20 kilometers.
This means that 40\% of the ensemble members have variability in their $\sigma_x$ parameter that was greater than 10\% of the median length of the $\sigma_x$ fitting parameter.


\mbox{Figure \ref{sigmay-25mm-400km-dist}} shows the box-and-whisker plots for the $\sigma_y$ fitting parameter.
This parameter is the length of the short axis of the fitted anisotropic Gaussian function.
Unlike with the $\sigma_x$ fitting parameter, the ensemble member distributions of the $\sigma_y$ fitting parameter exhibit fewer outliers with only four members having them.
This can be attributed to their being more variability in the 25th to 75th percentile, as noted by the increased size of the boxes for many members.
Furthermore, whereas when the maximum range of the $\sigma_x$ fitting parameter distribution greater than 20 kilometers indicated the presence of an outlier, several members have distributions of $\sigma_y$ with a maximum range greater than 20 kilometers without having an outlier.
In fact, six out of the ten WRF-ARW members have a range of $\sigma_y$ greater than 20 kilometers and only two of them have outliers.


The distributions of the counter-clockwise rotation angle of the abscissa, fitting parameter $\theta$, is shown in \mbox{Figure \ref{theta-25mm-400km-dist}}.
For this fitting parameter, eleven out of fifteen members exhibit outliers.
This is not really surprising when one considers the overlap in the distributions of fitting parameters $\sigma_x$ and $\sigma_y$.
As alluded to in \mbox{Section \ref{std}}, when $\sigma_x$ approaches $\sigma_y$, the Gaussian function approaches isotropy.
As a Gaussian function approaches isotropy the variability of the rotation angle, $\theta$ increases as a result of the more circular nature to the function.
It is much more difficult to accurately measure the orientation of the rotation angle of an ellipse that is nearly circular than that of one with a high level of eccentricity.


The large variability in the fitting parameters was not limited to just the shape of the anisotropic Gaussian.
The location of the fitting Gaussian also ranged widely (fitting parameters $\mu_x$ and $\mu_y$).
Each member's distributions of the $\mu_x$ fitting parameter (displacement in the east or west direction) is shown in \mbox{Figure \ref{mu_x-25mm-400km-dist}}.
Generally speaking, every SSEF member's distribution of $\mu_x$ varies by over 20 kilometers (approximately 4-5 grid points), with many members demonstrating an even wider range, and that's just the variability within each member's distribution of $\mu_x$.
Some of the values for $\mu_x$ deviated from the forecast grid point by over 40 kilometers, with the maximum difference between $\mu_x$ values between any two members being over 80 kilometers!


Specifically, ARPS and WRF-NMM members consistently demonstrated a bias toward the observations centroid being displaced eastward, which corresponds to the forecasts, on average, being too far west.
Most of the WRF-ARW members exhibited the opposite behavior, namely having the observations too far westward (indicating a eastward bias with the forecast).
Not every WRF-ARW member exhibited this bias, however.
Two of the WRF-ARW members have a majority of the distribution consist of positive values for $\mu_x$ indicating a westward forecast bias.
However, unlike with the ARPS and WRF-NMM members where the entire distribution consisted of positive values for $\mu_x$, every member of the WRF-ARW had at least a portion of the distribution with negative values, indicating an eastward forecast bias compared to observations.


Similar variability in the distributions of fitting parameter $\mu_y$, displacement in the north-south direction, are also observed (\mbox{Figure \ref{mu_y-25mm-400km-dist}}).
One difference, however, is that there appears to be a forecast displacement preference for most members.
Most members' forecasts appear to be farther north than the centroid of observations, which is opposite of the mean NSSL-WRF latitudinal displacement.


In addition to examining the two-dimensional composites in the context of their one-dimensional distributions of the five anisotropic Gaussian fitting parameters, a cursory examination of their spatial characteristics was also conducted.
The standard deviation of each member's two-dimensional composites is shown in \mbox{Figure \ref{ssef-25mm-400km-std}}.
A quick examination of the standard deviation of the two-dimensional composites does not yield any immediate insights.
Some members exhibit a maximum in variability along a southwest to northeast orientation, whereas other members exhibit a more uniform decrease in variability as one moves radially outward from the forecast grid point.
In both of these orientations, the maximum variability tended to be located near the forecast grid point, with generally decreasing variability as distance from the forecast grid point increases.


Unfortunately, \mbox{Figure \ref{ssef-25mm-400km-std}} doesn't yield much insight into what the specific two-dimensional composites used in the various simulations might look like.
To examine that aspect of the distributions, a single simulation was chosen at random from the twenty simulations to examine further.
\mbox{Figure \ref{ssef-25mm-400km-composite}} shows the two-dimensional composites for each member at the \mbox{25.4 mm} in \mbox{6 hr} threshold.
In this simulation, it is easily suggested that the WRF-NMM ensemble members are generally the wettest, as indicated by the substantially more observations, with the WRF-ARW members generally drier.
(The ARPS member is in between.)
The overall axis of observations relative to forecasts indicates a general southeast-to-northeast orientation in most members, although this is more readily apparent in some members than others.


The lower right panel of \mbox{Figure \ref{ssef-25mm-400km-std}} depicts the standard deviation between the two-dimensional composites of each member for all simulations.
In this panel, the darker colors indicate a greater standard deviation at that particular grid point than grid points with a lighter color.
There is considerable variability over the southern and eastern portion of the composite radius, with the maximum in variability at relatively far distances to the south, and slightly east.
The least variable portion region over the compositing radius is found to the north and northwest.


Although the orientation of the axis of maximum observations tends to generally be similar between members, the centroid of the distribution exhibits more variability.
About half of the members have the centroid of observations too far east and half being too far west.
(This corresponds to the $\mu_x$ parameter of the Gaussian fitting parameters.)
The members having the observations centroid too far east tended to be farther away from the forecast than those members having the observations centroid too far west.
In this simulation, the WRF-NMM has a westward bias with its forecasts, as the centroid of observations is east of the forecast grid point in every WRF-NMM member.
A similar observation cannot be made for the WRF-ARW members.
It is unclear from this single simulation if the westward forecast bias in the WRF-NMM members is systematic of the WRF-NMM core, or merely a function of only having 4 WRF-NMM members.


When examining the north-south variations of the observations centroid relative to the model forecast, similar variations are observed (not shown).
(The north-south displacement of the observations centroid corresponds to the $\mu_y$ parameter of the Gaussian fitting parameters.)
Slightly more than half the members have the centroid of observations too far north, indicating a southward bias of the forecast, with slightly less than half having the centroid of observations too far south.
Three of the four WRF-NMM members had the forecast too far north, with the WRF-NMM control member having the greatest displacement.
No obvious preference in displacement direction is readily apparent from the WRF-ARW members.
However, it does appear that generally speaking, the magnitude of the displacement of the WRF-ARW members appears to be less than that of the WRF-NMM members.


Similar to the lower right panel of \mbox{Figure \ref{ssef-25mm-400km-std}}, the lower right panel of \mbox{Figure \ref{ssef-25mm-400km-composite}} depicts the standard deviation between the two-dimensional composites of each member for the given simulation.
In this panel, the darker colors indicate a greater standard deviation at that particular grid point than grid points with a lighter color.
It is readily apparent that the greatest variability between the members exists to the east of the forecast grid point, as was the case with the overall variability denoted in the lower right panel of \mbox{Figure \ref{ssef-25mm-400km-std}}.
This is due to the wetter WRF-NMM members having a westward forecast bias, resulting in a wider range of observation counts to the east of the forecast.


Although there appears to be variability in the various anisotropic Gaussian functions used to model the displacement characteristics of the SSEF members, how does the modeled Gaussian work in practice?
\mbox{Figure \ref{ssef-25mm-400km-example}} shows the probabilistic forecast from each member of the SSEF for the six hours ending 06 UTC 20 May 2010.
Meteorologically speaking, this time period comes at end of a severe weather day; the SPC issued a High Risk for severe weather across portions of central Oklahoma for much of the day 19 May 2010.
Thunderstorms developed across northwestern Oklahoma and southern Kansas early in the afternoon and developed eastward from there.
Several clusters of thunderstorms emerged through the course of the afternoon, culminating in a heavy rain event across portions of eastern Oklahoma, eastern Kansas, western Missouri, and western Arkansas.
A large area of observed precipitation amounts exceeding the \mbox{25.4 mm} in \mbox{6 hr} threshold  occurred across eastern Oklahoma into western Arkansas and southwestern Missouri, with a smaller area observed across northeast Kansas into northwest Missouri.
Additionally, a small area exceeding the threshold was observed in southwest Kansas to the north of Dodge City, KS.


In terms of the overall appearance of their probabilistic forecasts, each member appears to produce a probabilistic forecast that is maximized in the vicinity of the largest area of observed precipitation accumulation, with decreasing probabilities to the north and south.
Furthermore, every member of the SSEF seems to have captured the general area where \mbox{25.4 mm} in \mbox{6 hr} occurred.
In fact, within the limited domain shown in \mbox{Figure \ref{ssef-25mm-400km-example}}, all observed areas exceeding the given threshold were captured by non-zero probabilities, with most areas, for most SSEF members, captured within at least a 5\% contour.
However, no members' probabilistic forecast encapsulated all observed areas with at least a 5\% contour.
Although each probabilistic forecast appears to highlight the same area for a heavy rain potential, the character of each member's probabilistic forecast is different.
All members appear to convey the threat of an elongated north-south area with the potential to see heavy rain, but a handful of members also suggest the possibility of the heavy rain extending northwestward into central and western Kansas.
The ARPS member and the WRF-ARW control member both appear to have their highest probabilities farther north than the other members, but still south of the secondary area of observed \mbox{25.4 mm} in \mbox{6 hr}.


The lower right panel of \mbox{Figure \ref{ssef-25mm-400km-example}} is the ``ensemble'' probability forecast.
It was computed by taking the average of all fifteen SSEF probabilities at each grid point.
Because each of the SSEF members highlight the same general location with their individual forecasts, the ensemble average appears to be a good forecast as well.
However, one drawback of an ensemble average forecast is that the potential exists for the resulting forecast to have relatively low amplitude and an overly smooth appearance.
This is because averaging tends to dampen the higher probabilities from each of the SSEF members due to the low likelihood of the maximum probabilities from each member actually occurring on the same grid point in all of the members.
Furthermore, if the individual probability forecasts were spatially very different, the ensemble average field would essentially average out all of the signal offered by the individual members.


As previously mentioned, it is incorrect to assess the validity of a probabilistic forecast on the basis of a single probabilistic forecast.
Instead, probabilistic forecasts must be evaluated over a sufficiently large sample of forecasts, such that each probability forecast is used a sufficient number of times to be accurately evaluated.
In an idealized world, probabilistic forecasts would be evaluated over an infinite number of forecasts.
Since an infinite number of forecasts is impossible, it is generally understood that the larger the sample of probabilistic forecasts to be evaluated, the more robust the verification of the probabilities will be.


Unfortunately, with only 60 forecast time periods for each of the SSEF members, and dealing with a rare event, drawing meaningful verification statistics is a challenge.
The approach used here was to create performance diagrams and reliability diagrams for each member (\mbox{Figure \ref{ssef-25mm-400km-verif}}).
Rather than plot the curves of all twenty simulations for all fifteen members to visualize the uncertainty in the verification, only the mean curve for each member is plotted.
In the case of a reliability diagram, visualizing the uncertainty of the mean reliability is easily achieved by plotting error bars of observed probability around the mean reliability at each members' unique forecast probability.
Here, the error bars, or uncertainty in the mean, were calculated to be $\pm$ one standard deviation of the observed probabilities of that member's forecast probability at that given probability threshold.
In an effort to prevent too many lines from cluttering the reliability diagram, instead of using error bars the area bounded by $\pm$ one standard deviation of each member's mean reliability is color filled with a semi-transparent color.)


Visualizing uncertainty on a performance diagram is more difficult.
The plotted point is based upon that forecast probability's Success Ratio (abscissa) and Probability of Detection (ordinate).
Since both of these are derived verification measures each can exhibit variability.
In other words, the location of each evaluated forecast probability is not fixed along either the abscissa or the ordinate.
Furthermore, instead of being able to add a single set of error bars, error bars must be added in both coordinate directions, making for a cluttered figure.
Once again, instead of adding error bars, which would make the figure even more difficult to read, an ellipse is drawn around each data point, where the ellipse's x-axis is given by the standard deviation in the abscissa and the ellipse's y-axis is given by the standard deviation in the ordinate.


\mbox{Figure \ref{ssef-25mm-400km-verif}} depicts both the performance diagram (panel a) and the reliability diagram (panel b) for the SSEF ensemble at the \mbox{25.4 mm} threshold.
Looking at the performance diagram it is readily apparent that the overall values appear to be worse than those from the NSSL-WRF in \mbox{Figure \ref{single_verif_400km_25mm}}, with CSI scores being less than 0.1 for all probability thresholds from all ensemble members.
Visually, it does not appear there is much difference between the individual SSEF member forecasts (black curves) and the ensemble forecast generated by a modified Hamill and Colucci method (red curve; \citealp{Hamill1998, Clark2009}), as the latter forecasts appear to fall within the spread of SSEF member forecasts.
It does appear as if there is a slight improvement by using the ensemble mean forecast (blue curve), but it does not offer much improvement.
Looking at the ``error clouds'' associated with each point, it does not look like there is much variability in the performance diagram plots between each simulation for each SSEF member.


Examining the reliability diagrams found in \mbox{Figure \ref{ssef-25mm-400km-verif}b} yields a slightly different story.
While the performance diagram suggests the forecast ``goodness'' was generally poor, the reliabilities appear to be fairly good for such a rare event and such a small sample size.
By no means is the reliability near perfect for the SSEF member forecasts, but except for the rarer probability forecasts of some SSEF members (i.e., higher probability forecasts), every member of the SSEF falls between perfect reliability and the line of no skill (in a Brier Score sense; not shown).
The variability of the reliability diagram to changes in simulation appears to be greater at higher forecast probabilities, of which their are fewer forecast grid points to evaluate.
This is easily seen in the general increase in width of the error plumes surrounding the mean reliability as the forecast probabilities increase.
It should also be noted that, except for the rare probability threshold forecasts, it appears that every member of the SSEF is closer to perfect reliability than the forecasts generated by the modified Hamill and Colucci method.
However, one benefit of the modified Hamill and Colucci method forecasts is that probability forecasts occur over a wider range of forecast thresholds.
In other words, it is possible to have a near 100\% probability forecast, that is currently not possible from the individual SSEF member forecasts.


One thing of note in the reliability diagrams is that most probabilistic forecasts, for every member, including the ensemble mean (blue curve) and modified Hamill and Colucci generated forecasts, is an over forecast.
In other words, the number of grid point observations at a given forecast probability is fewer than the number of observations needed to achieve perfect reliability for that particular member.
One interesting artifact of the over forecasts by every member of the SSEF is that the ensemble average probability forecasts appear to be closer to perfect reliability than any of the individual SSEF members.
This is because the averaging process acts as a spatial smoother, damping the peak probabilities, and spreading them out over a slightly larger area.
Averaging a set of probability forecasts has the affect of shifting the resulting reliability curve to the left, as compared to the collection of reliability curves for each individual member.
Thus, because every SSEF member over forecasts, the resulting damped, and spatially smoothed forecasts caused by averaging will be closer to perfect reliability than the individual forecasts\footnote{This is not always guaranteed. In the limiting case of every ensemble member producing the same exact forecast, the ensemble average will be identical to the collection of ensemble forecasts.}.




\subsection{12.7 mm Threshold}
\label{eresults_12.7mm}

As was the case with the NSSL-WRF, the analysis of the proposed calibration method was also carried out at the \mbox{12.7 mm} in \mbox{6 hr} threshold using the SSEF data.
This was partly motivated by the high variability in both the two-dimensional composites and the resulting reliability curves.
By examining the proposed technique at a lower precipitation accumulation threshold, it is hoped that more forecast events (grid points) will occur.
The increase in precipitation events would allow for more filled out two-dimensional composites, and hopefully decrease the intra member variability in the two-dimensional composites between simulations.
This should, in turn, provide for a larger sample from which to conduct the verification and evaluate the probabilistic forecasts.
The format here follows the same as with the \mbox{25.4 mm} in \mbox{6 hr} threshold discussed in \mbox{Section \ref{eresults_25.4mm}}.


As was the case before, a set of five figures (\mbox{Figures \ref{sigmax-12mm-400km-dist}--\ref{mu_y-12mm-400km-dist}}) were created to examine the variability of each of the five Gaussian fitting parameters --- one figure per fitting parameter.
\mbox{Figure \ref{sigmax-25mm-400km-dist}} displays the box-and-whisker plots for the $\sigma_x$ fitting parameter.
This parameter is the length of the long axis of the fitted anisotropic Gaussian function.
Fairly important differences arise when comparing the distributions of the $\sigma_x$ fitting parameter between the \mbox{25.4 mm} threshold and the \mbox{12.7 mm} threshold.
In the case of the \mbox{25.4 mm} in \mbox{6 hr} threshold, six members had distributions in which the range of $\sigma_x$ values exceeded 20 kilometers.
With the lower threshold of \mbox{12.7 mm} in \mbox{6 hr} the range between the largest and smallest values of $\sigma_x$ is less than 18 kilometers.
In fact, most members have a range of $\sigma_x$ values that is less than 10 kilometers.


Similar results are found when examining the box-and-whisker plots for the $\sigma_y$ fitting parameter (\mbox{Figure \ref{sigmay-25mm-400km-dist}}) as were found with the $\sigma_x$ distributions.
Once again the maximum range between any two $\sigma_y$ values from any member is approximately 15 kilometers, which is about half the maximum difference between any two $\sigma_y$ values at the \mbox{25.4 mm} threshold; most members exhibit a range of less than 10 kilometers.
Furthermore, whereas the distributions of $\sigma_x$ and $\sigma_y$ had quite a bit of overlap at the \mbox{25.4 mm} threshold, there is no overlap between the two fitting parameters at the \mbox{12.7 mm} threshold.
It is worth noting that six of the fifteen members exhibit an outlier value for $\sigma_y$ at the \mbox{12.7 mm} threshold, but even those members exhibit a range of less than 15 kilometers.


The distributions of the counter-clockwise rotation angle of the abscissa, fitting parameter $\theta$, is shown in \mbox{Figure \ref{theta-12mm-400km-dist}}.
For this fitting parameter, six out of fifteen members exhibit outliers, which is nearly half the number of members with outliers at the \mbox{25.4 mm} threshold.
This is not really surprising when one considers the lack of any overlap in the distributions of fitting parameters $\sigma_x$ and $\sigma_y$ at the \mbox{12.7 mm} threshold.
Recall when $\sigma_x$ approaches $\sigma_y$, the Gaussian function approaches isotropy, resulting in nearly circular distributions that make it difficult to assess the angle of rotation.
With no overlap in the distributions of $\sigma_x$ and $\sigma_y$, this ensures that the resulting two-dimensional composites of some degree of eccentricity and a somewhat better defined abscissa rotation angle.
Even the SSEF members that do have outliers, the range of $\theta$ values is still significantly less than the variability seen in $\theta$ at the \mbox{25.4 mm} threshold.
In fact, the median rotation angle ($\theta$) from all of the members appears to be within 5-7 degrees of 45\degree, with the maximum orientation being slightly more than 70\degree and the minimum orientation being slightly less than 30\degree.
This means that the fitted anisotropic Gaussian for all two-dimensional composites at the \mbox{12.7 mm} threshold exhibits some variation on a southwest-to-northeast orientation.


The improvement in the variability in the fitting parameters was not limited to just the shape of the fitted anisotropic Gaussian.
The variability in the location of the fitting Gaussian also decreased (fitting parameters $\mu_x$ and $\mu_y$), although not by as much as with $\sigma_x$, $\sigma_y$, and $\theta$.
Each member's distributions of the $\mu_x$ fitting parameter (displacement in the east or west direction) is shown in \mbox{Figure \ref{mu_x-12mm-400km-dist}}.
Generally speaking, every SSEF member's distribution of $\mu_x$ varies by near, or slightly less than, 20 kilometers (approximately 4 grid points).


Specifically, ARPS and WRF-NMM members consistently demonstrated a bias toward the observations centroid being east, which corresponds to the forecasts, on average, being too far west.
Most of the WRF-ARW members exhibited the opposite behavior, namely having the observations too far west (indicating an eastward bias with the forecast).
Not every WRF-ARW member exhibited this bias, however.
Two of the WRF-ARW members have a majority of the distribution consist of positive values for $\mu_x$ indicating a westward forecast bias.
However, unlike with the ARPS and WRF-NMM members where the entire distribution consisted of positive values for $\mu_x$, every member of the WRF-ARW had at least a portion of the distribution with negative values, indicating an eastward forecast bias compared to observations.
Similar variability in the distributions of fitting parameter $\mu_y$, displacement in the north-south direction, are also observed (\mbox{Figure \ref{mu_y-12mm-400km-dist}}).
One difference, however, is that there appears to be a forecast displacement preference for most members.
Most members' forecasts appear to be farther north than the centroid of observations.
These results are generally consistent with the distributions of $\mu_x$ and $\mu_y$ at the \mbox{25.4 mm} threshold, albeit with slightly more restricted distributions.


Next, the two-dimensional composites spatial characteristics are examined.
The standard deviation of each member's two-dimensional composites is shown in \mbox{Figure \ref{ssef-12mm-400km-std}}.
A quick examination of the standard deviation of the two-dimensional composites shows that the two-dimensional composites appear to be more uniform than those for the \mbox{25.4 mm} threshold.
In other words, the general pattern to the standard deviations suggests that there is not any obvious spatial biases.
It does appear that the WRF-NMM members have more variability to the east of the representative forecast grid point, and that there may be more of a maximum in variability from the southwest-to-northeast for these members, but is not as obvious as shown in \mbox{Figure \ref{ssef-25mm-400km-std}}.
In all standard deviation plots the maximum variability tends to be located near the forecast grid point, with generally decreasing variability as distance from the forecast grid point increases.


The lower right panel of \mbox{Figure \ref{ssef-12mm-400km-std}} depicts the standard deviation between the two-dimensional composites of each member for all simulations.
In this panel, the darker colors indicate a greater standard deviation at that particular grid point than grid points with a lighter color.
There is considerable variability over the southeastern and eastern portion of the composite radius, with the maximum in variability at relatively far distances to the south, and slightly east.
The least variable portion region over the compositing radius is found to the north, south, and west, with decreasing variability the farther away from the forecast grid point.


Unfortunately, \mbox{Figure \ref{ssef-12mm-400km-std}} doesn't yield much insight into what the specific two-dimensional composites used in the various simulations might look like.
To examine that aspect of the distributions, the same simulation used in \mbox{Figure \ref{ssef-25mm-400km-composite}} was used to examine further.
\mbox{Figure \ref{ssef-12mm-400km-composite}} shows the two-dimensional composites for each member at the \mbox{12.7 mm} in \mbox{6 hr} threshold.
In this simulation, it is easily suggested that the WRF-NMM ensemble members are generally the wettest, as indicated by the increased number of observations, with the WRF-ARW members generally drier.
(The ARPS member is still in between.)
The overall axis of observations relative to forecasts indicates a general southeast-to-northeast orientation in most members, although this is more readily apparent in some members than others.


The lower right panel of \mbox{Figure \ref{ssef-12mm-400km-std}} depicts the standard deviation between the two-dimensional composites of each member for all simulations.
In this panel, the darker colors indicate a greater standard deviation at that particular grid point than grid points with a lighter color.
There is considerable variability over the southern and eastern portion of the composite radius, with the maximum in variability at relatively far distances to the south, and slightly east.
The least variable portion region over the compositing radius is found to the north, west, and far south.


Although the orientation of the axis of maximum observations tends to generally be similar between members, the centroid of the distribution exhibits more variability.
About two-thirds of the members have the centroid of observations too far east and one-third have the centroid be too far west.
(This corresponds to the $\mu_x$ parameter of the Gaussian fitting parameters.)
This represents an increase in the number of members where the centroid of observations is found to be to the east.
The members having the observations centroid too far east tended to be farther away from the forecast than those members having the observations centroid too far west.
This is consistent with the findings at the \mbox{25.4 mm} threshold, although the gap between the two groups of members has decreased.
In this simulation, the WRF-NMM has a westward bias with its forecasts, as the centroid of observations is east of the forecast grid point in every WRF-NMM member.
A similar observation cannot be made for the WRF-ARW members.
As was also the case at the \mbox{25.4 mm} threshold, it is unclear from this single simulation if the westward forecast bias in the WRF-NMM members is systematic of the WRF-NMM core, or merely a function of only having 4 WRF-NMM members.


When examining the north-south variations of the observations centroid relative to the model forecast, similar variations are observed.
(The north-south displacement of the observations centroid corresponds to the $\mu_y$ parameter of the Gaussian fitting parameters.)
Slightly more than half the members have the centroid of observations too far south, indicating a northward bias of the forecast, with slightly less than half having the centroid of observations too far north.
This is the opposite that at the \mbox{25.4 mm} threshold.
Three of the four WRF-NMM members had the forecast too far north, with the WRF-NMM S4M4 member having the least displacement.
No obvious preference in displacement direction is readily apparent from the WRF-ARW members.
However, it does appear that generally speaking, the magnitude of the displacement of the WRF-ARW members appears to be less than that of the WRF-NMM members.


Similar to the lower right panel of \mbox{Figure \ref{ssef-12mm-400km-std}}, the lower right panel of \mbox{Figure \ref{ssef-12mm-400km-composite}} depicts the standard deviation between the two-dimensional composites of each member for the given simulation.
In this panel, the darker colors indicate a greater standard deviation at that particular grid point than grid points with a lighter color.
It is readily apparent that the greatest variability between the members exists to the east of the forecast grid point, as was the case with the overall variability denoted in the lower right panel of \mbox{Figure \ref{ssef-12mm-400km-std}}.
This is due to the wetter WRF-NMM members having a westward forecast bias, resulting in a wider range of observation counts to the east of the forecast.


How does the modeled Gaussian work in practice?
\mbox{Figure \ref{ssef-12mm-400km-example}} shows the probabilistic forecast from each member of the SSEF for the six hours ending 06 UTC 20 May 2010, the same as in \mbox{Figure \ref{ssef-25mm-400km-example}}.
At the lower precipitation accumulation threshold of \mbox{12.7 mm} in \mbox{6 hr}, a large area of observed precipitation amounts exceeding the threshold occurred across eastern Oklahoma and western Arkansas and extended north and northeastward into western and central Missouri as well as eastern and northeastern Kansas.
Additionally, small areas exceeding the threshold were observed in northeast Louisiana and extreme southeastern Arkansas, south-central into southwestern Kansas, northern Texas Panhandle, eastern Colorado, and southeastern Wyoming.


In terms of the overall appearance, each member appears to produce a probabilistic forecast that is maximized in the vicinity of the largest area observed precipitation accumulation exceeding the specified threshold, with decreasing probabilities to the north and south.
Furthermore, every member of the SSEF seems to have captured the general area where \mbox{12.7 mm} in \mbox{6 hr} occurred.
In fact, as was the case at the higher precipitation threshold, within the limited domain shown in \mbox{Figure \ref{ssef-12mm-400km-example}}, all observed areas exceeding the given threshold were captured by non-zero probabilities, with most areas, for most SSEF members, captured within at least a 5\% contour.
However, no members' probabilistic forecast encapsulated all observed areas with at least a 5\% contour.
Although each probabilistic forecast appears to highlight the same area for a heavy rain potential, the character of each member's probabilistic forecast is different.
All members appear to convey the threat of an elongated north-south area with the potential to see heavy rain, with most members also suggesting the possibility of the heavy rain to extend northwestward into central and western Kansas.
None of the members captured the observed area in northeast Louisiana, nor the areas in the northern Texas Panhandle and southeastern Wyoming.
It is tempting to say the ensemble forecasts are poor in these areas, however, that cannot be determined based on the figures shown.
This is because the calibration approach presented here requires a large number of nearby grid points forecast to exceed the specified threshold in order produce a probabilistic forecast that exceeds the 5\% contour.
It is quite possible that the individual SSEF members did accurately predict accumulations in these ``missed'' areas that exceeded the \mbox{12.7 mm} in \mbox{6 hr} threshold, but did so over a small area which yielded small forecast probabilities.


Lastly, the lower right panel of \mbox{Figure \ref{ssef-12mm-400km-example}} is the ``ensemble'' probability forecast.
Because each of the SSEF members highlight the same general location with their individual forecasts, the ensemble average appears to be a good forecast also.


\mbox{Figure \ref{ssef-12mm-400km-verif}} displays the performance and reliability diagram for the \mbox{12.7 mm} in \mbox{6 hr} threshold.
Looking at the performance diagram (\mbox{Figure \ref{ssef-12mm-400km-verif}a}) it can be seen that most curves have shifted to the upper-right, indicating improved forecasts as compared to the \mbox{25.4 mm} forecasts.
As was the case at the great accumulation threshold, there is little variability in the curves for an individual member, as indicated by the small ``error cloud'' surrounding each plotted point.
The error clouds do increase as one moves left-to-right along the curve (i.e., as one moves from lower forecast probability to higher forecast probability), which is expected as the higher forecast probabilities are forecasted fewer times.
Once again the forecast probabilities generated by the modified Hamill and Colucci method (red curve) are shown to perform slightly better over a larger range of forecast probabilities as compared to the individual SSEF member forecasts.
The SSEF ensemble generated probabilities (blue curve) are similar to those from the modified Hamill and Colucci method over most of the diagram, but perform slightly better at some of the higher forecast probabilities.


Examining the reliability diagram (\mbox{Figure \ref{ssef-12mm-400km-verif}b}) much better results are shown as compared to the reliability diagrams constructed for the \mbox{25.4 mm} forecasts.
For all SSEF forecasts, the range over which probability forecasts are generated has increased from a maximum of just under 50\% for the higher members at the \mbox{25.4 mm} threshold to just under the 70\% at the \mbox{12.7 mm} threshold.
It's also readily seen that the SSEF member reliability curves are much closer to perfect reliability than before.
In fact, with the exception of each member's highest value probability forecasts (right most portions of the black curves), the SSEF member's reliability is generally within 10\% of the observed probabilities.


As was the case at the \mbox{25.4 mm} threshold, the forecast probabilities generated by the modified Hamill and Colucci method are the worst performing forecasts, strictly in terms of reliability.
Over most probability forecasts, the reliability curve for the modified Hamill and Colucci forecasts is well separated from the reliability curves constructed from the probabilistic forecasts generated directly from each ensemble member.
One thing that is import to note, is that unlike what occurred with the forecasts at the \mbox{25.4 mm} threshold, the ensemble averaged probabilities are \emph{less} reliable than the individual SSEF member forecasts.
Upon reflection, this is not surprising. As explained toward the end of \mbox{Section \ref{dresults_25.4mm}}, the act of averaging the ensemble forecasts results in a reliability curve being shifted to the left as compared to the reliability curves constructed from the individual members that make up the ensemble.
Since the reliability curves for each SSEF member are near perfect reliability to begin with, constructing a forecast from the average forecast probabilities will result in that forecast's reliability curve to shift to the left and under forecast the observed probability at each forecast probability.




\subsection{Discussion}
\label{ediscussion}

It is evident from examining \mbox{Figures \ref{sigmax-25mm-400km-dist}--\ref{ssef-25mm-400km-composite} and \ref{sigmax-12mm-400km-dist}--\ref{ssef-12mm-400km-composite}} that there is a wide range of variability amongst the fitting parameters at all precipitation accumulation thresholds.
Furthermore, examining the verification plots (\mbox{Figures \ref{ssef-25mm-400km-verif}a,b and \ref{ssef-12mm-400km-verif}a,b}) could lead one to believe that the probabilistic forecasts are not very good.
However, it is important to take these results in proper context.




\subsubsection{Warm Season verus Cool Season}

The period from late Spring through Summer is characteristically the hardest time for numerical weather prediction with respect to quantitative precipitation forecasts!
To illustrate this point, the \mbox{12.7 mm} (\mbox{0.5 in}) in \mbox{6 hr} quantitative precipitation forecast verification scores of the NAM and Global Forecast System (GFS) numerical models, as well as the human forecasts produced by the NOAA Hydrometeorological Prediction Center (HPC; Available online at \url{http://www.hpc.ncep.noaa.gov/html/hpcverif.shtml#6hour}) are shown in \mbox{Figures \ref{hpc18}--\ref{hpc36}} .
The figures show the threat score (equivalent to CSI) of each forecast system plotted as a function of month for the same forecast time periods as used in both the NSSL-WRF and the SSEF.
The general trend is for a decline in forecast verification scores during the warm season (late Spring through Summer) and an increase in the cool season (late Autumn through Winter).
Although these verification scores are for the year 2012, a different time period than used in the analyses conducted in this paper and are only valid at a single precipitation accumulation threshold, the general trend is similar to previous years and higher accumulation thresholds.


The evaluation of the calibration method, as applied to the SSEF, has been conducted at what is traditionally one of the most challenging times for quantitative precipitation forecasting, both for numerical weather prediction and for humans!
Bearing this in mind, a closer look at the performance diagram of the \mbox{12.7 mm} in \mbox{6 hr} threshold indicates that the CSI scores at the various probabilistic forecast values for the SSEF members are slightly better than the CSI values from the larger-scale numerical models (GFS and NAM) for the corresponding time of year.
This means that it is possible to construct a deterministic forecast from the probabilistic forecast of any SSEF member that is more skillful than a deterministic forecast from the GFS or NAM.
This is achieved for any SSEF member by choosing a forecast probability threshold for that member that exhibits a higher CSI than either the NAM or GFS and converting all forecast probabilities less than that threshold into a ``NO'' forecast and forecast probabilities exceeding that threshold into a ``YES'' forecast.
Here a ``NO'' forecast indicates that the grid point is not expected to exceed the specified threshold, and a ``YES'' forecast indicates that the grid point is expected to exceed the specified threshold.
In fact, for some members of the SSEF, it would be possible to generate a deterministic forecast from the probabilistic forecast that would be comparable in skill to the human forecasts produced by HPC\footnote{HPC does not verify their quantitative precipitation forecasts on a grid point by grid point basis.
Instead they verify at a number of predefined locations.
Thus, comparison of HPC's verification to the verification of the proposed method is not one-to-one and should be considered preliminary.
Please see \cite{Olsen1995} for additional information on how HPC conducts its verification.}.


It is important to note that this more skillful deterministic forecast can be created from any of the SSEF members.
In other words, it is possible to produce a collection of deterministic forecasts, derived from probabilistic forecasts, that are more skillful than the larger-scale numerical models.
The fact the more skillful deterministic forecasts can be derived from the probabilistic forecasts is an important one.
This allows for improvements in the current deterministic paradigm in which a majority of end users operate, but it also provides additional useful information for higher-end users.
End users who have very specific cost-loss thresholds can utilize the probabilistic forecast to gain additional, user-specific information from the probabilistic forecasts, without negatively impacting users who depend on the legacy deterministic forecasts.




\subsubsection{Limited Training Period}


With only 59 time periods from which to try and capture each SSEF member's displacement characteristics, achieving the threshold of \mbox{25.4 mm} in \mbox{6 hr} occurs too infrequently to create a two-dimensional composite from which to calculate robust fitting parameters.
The 59 training time periods is simply not long enough to adequately capture the variability in each member's spatial displacement for such a rare event.
For instance, the maximum number of observations at any of the two-dimensional composite grid points, from any SSEF member, is less than 25 000.
Compare that with the hundreds of thousands found in the NSSL-WRF composite at the same threshold.
This inability to capture all of the displacement variability for all SSEF members significantly undercuts the justification of the proposed calibration method.
Namely, that by modeling the displacement errors, and using the modeled displacement errors as the basis of KDE, one can create calibrated probabilistic forecasts from deterministic models.
Unfortunately, the method is only as good as the ability to correctly model the underlying displacement errors.
If the resulting two-dimensional composite is unable to adequately represent the displacement error, the method will not perform optimally.


In the case of the SSEF, simply moving to a lower threshold, such as the \mbox{12.7 mm} in \mbox{6 hr}, resulted in the maximum number of observations at any two-dimensional composite grid point being greater than one hundred thousand -- even for such a limited time period!
This allowed for an a better depiction of the underlying displacement error, and a much more reliable probabilistic forecast was achieved.
